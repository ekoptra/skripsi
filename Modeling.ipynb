{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import os, json, pickle\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from helpers.DataSaham import DataSaham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model yang akan digunakan adalah multi-step timeseries. Model tersebut akan memprediksi 5 hari kedepan dengan pendekatan one vector output (bukan auto regressive). Model tersebut akan dilatih setiap hari agar model tersebut bisa mempelajari pola dengan menambahkan data yang baru. Oleh karena itu tujuan dari pemodelan kali ini adalah mencari model dengan arsitekturnya yang paling bagus performanya seiiring bertambahnya waktu. Karena tidak mungkin setiap bertambahnya waktu dilakukan hyperparameter tuning yang sangat memakan waktu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_1  col_2  col_3  col_4  col_5  col_6  col_7  col_8  col_9  target\n",
       "0      1      2      3      4      5      6      7      8      9      10\n",
       "1     11     12     13     14     15     16     17     18     19      20\n",
       "2     21     22     23     24     25     26     27     28     29      30\n",
       "3     31     32     33     34     35     36     37     38     39      40\n",
       "4     41     42     43     44     45     46     47     48     49      50"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# buat data dummy dengan 10 variabel dan 100 observasi yang berurutan\n",
    "\n",
    "n_var  = 10\n",
    "n_row = 100\n",
    "\n",
    "col_names = [f\"col_{i+1}\" for i in range(n_var - 1)] + ['target']\n",
    "dummy = pd.DataFrame(\n",
    "    np.add(np.arange(n_var * n_row), 1).reshape(n_row, n_var),\n",
    "    columns=col_names\n",
    ")\n",
    "dummy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSMultistepSplit():\n",
    "    def __init__(self, n_splits, n_step, look_back):\n",
    "        self.n_splits = n_splits\n",
    "        self.n_step = n_step\n",
    "        self.look_back = look_back\n",
    "        \n",
    "    def split(self, X):\n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "        n_train = n_samples - (self.n_step + self.n_splits) + 1\n",
    "\n",
    "        if n_train < self.n_step + self.look_back + self.n_splits:\n",
    "            print(\"Sample size don't enough to make train data\")\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            end_train = n_train + i\n",
    "            yield indices[:end_train], indices[end_train-self.look_back:end_train + self.n_step]\n",
    "        \n",
    "\n",
    "class DataStore():\n",
    "    def __init__(\n",
    "        self, data, target_column, \n",
    "        look_back, n_steps, format=None,\n",
    "        scaler_x=None, scaler_y=None, \n",
    "        default_scaler=MinMaxScaler\n",
    "    ):\n",
    "        \n",
    "        self.data = data.copy()\n",
    "        self.target_column = target_column\n",
    "        self.look_back = look_back\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = data.shape[1] - 1\n",
    "        self.scaler_x = default_scaler() if scaler_x is None else scaler_x\n",
    "        self.scaler_y = default_scaler() if scaler_y is None else scaler_y\n",
    "        self.defined_scaler_x = scaler_x is not None\n",
    "        self.defined_scaler_y = scaler_y is not None\n",
    "        \n",
    "        if format == \"rnn\": self.format_for_rnn()\n",
    "        \n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as outp:\n",
    "            pickle.dump(self, outp, pickle.HIGHEST_PROTOCOL)\n",
    "               \n",
    "    def format_for_rnn(self):\n",
    "        (x, scaled_x), (y, scaled_y) = self.__get_independent_dependent_data(self.data, self.target_column)\n",
    "        \n",
    "        self.x, self.y = self.__lstm_output_vector(x, y)\n",
    "        self.scaled_x, self.scaled_y = self.__lstm_output_vector(scaled_x, scaled_y)\n",
    "        \n",
    "    def __get_independent_dependent_data(self,\n",
    "                                       data : pd.DataFrame, \n",
    "                                       target_column : str) -> tuple:\n",
    "\n",
    "        x = np.array(data)\n",
    "        y = np.array(data[target_column]).reshape(-1, 1)\n",
    "        \n",
    "        if not self.defined_scaler_x: self.scaler_x = self.scaler_x.fit(x)\n",
    "        if not self.defined_scaler_y: self.scaler_y = self.scaler_y.fit(y)\n",
    "        \n",
    "        scaled_x = self.scaler_x.transform(x)\n",
    "        scaled_y = self.scaler_y.transform(y).squeeze()\n",
    "        \n",
    "        return (x, scaled_x), (y, scaled_y)\n",
    "    \n",
    "    def __lstm_output_vector(self, data_x: np.ndarray,  data_y: np.ndarray) -> tuple:\n",
    "        x = []\n",
    "        y = []\n",
    "        n_data = len(data_x)\n",
    "        for index in range(n_data):\n",
    "            index_end = index + self.look_back\n",
    "            index_end_output = index_end + self.n_steps\n",
    "            \n",
    "            if index_end_output > n_data: break\n",
    "            \n",
    "            x.append(data_x[index:index_end, :])\n",
    "            y.append(data_y[index_end:index_end_output])\n",
    "\n",
    "        return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTuner(kt.Tuner):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def __get_folder(self, trial_id, fname):\n",
    "        return os.path.join(self.get_trial_dir(trial_id), fname)\n",
    "        \n",
    "    def save_model(self, trial_id, model, step=0):\n",
    "        fname = self.__get_folder(trial_id, \"model.h5\")\n",
    "        model.save(fname)\n",
    "        \n",
    "    def _save_data(self, trial_id, data_store, type_data, step=0):\n",
    "        fname = self.__get_folder(trial_id, f\"data_{type_data}.pkl\")\n",
    "        data_store.save(fname)\n",
    "        \n",
    "    def _save_metrics(self, trial_id, metrics):\n",
    "        fname = self.__get_folder(trial_id, \"metrics.json\")\n",
    "        with open(fname, 'w') as file:\n",
    "            file.write(json.dumps(metrics))\n",
    "            \n",
    "    def __load_model(self, trial_id):\n",
    "        fname = self.__get_folder((trial_id), \"model.h5\")\n",
    "        return tf.keras.models.load_model(fname)\n",
    "\n",
    "    def load_model(self, trial):\n",
    "        return self.__load_model(trial.trial_id)\n",
    "    \n",
    "    def _get_callbacks(self):\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=25, restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=15, min_lr=1e-7\n",
    "        )\n",
    "        \n",
    "        return [reduce_lr, early_stopping]\n",
    "    \n",
    "    def _data_store(\n",
    "        self, data, index, look_back,\n",
    "        scaler_x=None, scaler_y=None\n",
    "    ):\n",
    "        return DataStore(\n",
    "            data=data.iloc[index, :],\n",
    "            target_column=self.target_column,\n",
    "            look_back=look_back,\n",
    "            n_steps=self.n_steps,\n",
    "            scaler_x=scaler_x,\n",
    "            scaler_y=scaler_y,\n",
    "            format=\"rnn\"\n",
    "        )\n",
    "                \n",
    "    def get_metrics(self, errors):\n",
    "        squared_errors = np.power(errors, 2)\n",
    "        abs_errors = np.abs(errors)\n",
    "        return {\n",
    "            'rmse_total' : np.sqrt(squared_errors.mean()),\n",
    "            'rmse_eachday' : np.sqrt(squared_errors.mean(axis=0)).tolist(),\n",
    "            'mae_total' : abs_errors.mean(),\n",
    "            'mae_eachday' : abs_errors.mean(axis=0).tolist()\n",
    "        }\n",
    "        \n",
    "    def get_file_in_directory(self, best_trial=True, trial_id=None):\n",
    "        if best_trial:\n",
    "            trial_id = self.oracle.get_best_trials()[0].trial_id\n",
    "        \n",
    "        path_data_train = self.__get_folder(trial_id, \"data_train.pkl\")\n",
    "        with open(path_data_train, 'rb') as pickle_file:\n",
    "            data_train = pickle.load(pickle_file)\n",
    "            \n",
    "        path_data_test = self.__get_folder(trial_id, \"data_test.pkl\")\n",
    "        with open(path_data_test, 'rb') as pickle_file:\n",
    "            data_test = pickle.load(pickle_file)\n",
    "            \n",
    "        path_metrics = self.__get_folder(trial_id, \"metrics.json\") \n",
    "        with open(path_metrics, 'r') as file:\n",
    "            metrics = json.load(file)\n",
    "        \n",
    "        model = self.__load_model(trial_id)\n",
    "        return model, data_train, data_test, metrics\n",
    "        \n",
    "\n",
    "class WFVTuner(MyTuner):\n",
    "    def __init__(self, target_column, n_splits, n_steps, **kwargs):\n",
    "        self.target_column = target_column\n",
    "        self.n_splits = n_splits\n",
    "        self.n_steps = n_steps\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def __get_data(self, data, train_indices, test_indices, look_back):\n",
    "        data_train = self._data_store(data, train_indices, look_back)\n",
    "        data_test = self._data_store(\n",
    "            data, test_indices, look_back,\n",
    "            scaler_x=data_train.scaler_x,\n",
    "            scaler_y=data_train.scaler_y\n",
    "        )\n",
    "        return data_train, data_test\n",
    "    \n",
    "    def get_prediction(self, model, x, scaler=None):\n",
    "        prediction = model.predict(x)\n",
    "        if scaler is not None:\n",
    "            prediction = np.array(prediction).reshape(-1, 1)\n",
    "            prediction = scaler.inverse_transform(prediction)\n",
    "        return prediction\n",
    "    \n",
    "    def run_trial(self, trial, x, y, *args, **kwargs):\n",
    "        look_back = trial.hyperparameters.Choice('look_back', [5, 10, 20, 30])    \n",
    "        batch_size = trial.hyperparameters.Int('batch_size', 0, 64, step=8)\n",
    "        epochs = 100\n",
    "        \n",
    "        tss = TSMultistepSplit(\n",
    "            n_splits=self.n_splits, n_step=self.n_steps, look_back=look_back\n",
    "        )\n",
    "\n",
    "        errors = []\n",
    "        for train_indices, test_indices in tss.split(x):\n",
    "            data_train, data_test = self.__get_data(\n",
    "                x, train_indices, test_indices, look_back\n",
    "            )\n",
    "            \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            model.fit(\n",
    "                data_train.scaled_x, data_train.scaled_y, \n",
    "                shuffle=False, batch_size=batch_size,\n",
    "                epochs=epochs, verbose=2, \n",
    "                validation_data=(data_test.scaled_x, data_test.scaled_y), \n",
    "                callbacks=self._get_callbacks()\n",
    "            )\n",
    "            \n",
    "            prediction = self.get_prediction(model, data_test.scaled_x, data_test.scaler_y)\n",
    "            error = (prediction - data_test.y).reshape(-1)\n",
    "            errors.append(error)\n",
    "            \n",
    "        errors = np.array(errors)\n",
    "        metrics = self.get_metrics(errors)\n",
    "        \n",
    "        trial_id = trial.trial_id\n",
    "        self.oracle.update_trial(trial_id, {'val_rmse': metrics['rmse_total']})\n",
    "        self.save_model(trial_id, model)\n",
    "        self._save_data(trial_id, data_train, \"train\")\n",
    "        self._save_data(trial_id, data_test, \"test\")\n",
    "        \n",
    "        metrics['errors'] = errors.tolist()\n",
    "        self._save_metrics(trial_id, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(kt.HyperModel):\n",
    "    def __init__(self, n_features, n_steps, **kwargs):\n",
    "        self.n_features = n_features\n",
    "        self.n_steps = n_steps\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, hp):\n",
    "        \n",
    "        try:\n",
    "            look_back = hp.get('look_back')\n",
    "        except:\n",
    "            look_back = 10\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(\n",
    "            units=hp.Int('units', 100, 500, step=100),\n",
    "            input_shape=(look_back, self.n_features),\n",
    "            activation='tanh'\n",
    "        ))\n",
    "        \n",
    "        model.add(Dense(self.n_steps))\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss=\"mse\",\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10\n",
    "n_features = dummy.shape[1]\n",
    "\n",
    "hypermodel=MyHyperModel(\n",
    "    n_features=n_features,\n",
    "    n_steps=n_steps\n",
    ")\n",
    "\n",
    "oracle = kt.oracles.BayesianOptimization(\n",
    "    objective=kt.Objective('val_rmse', 'min'), \n",
    "    max_trials=3\n",
    ")\n",
    "\n",
    "tuner = WFVTuner(\n",
    "    target_column='target',\n",
    "    n_splits=10,\n",
    "    n_steps=n_steps,\n",
    "    oracle=oracle, \n",
    "    hypermodel=hypermodel,\n",
    "    directory=\"model\",\n",
    "    project_name=\"one_vector\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 01m 59s]\n",
      "val_rmse: 28.060833827405702\n",
      "\n",
      "Best val_rmse So Far: 17.28286298322715\n",
      "Total elapsed time: 00h 04m 57s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(dummy, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-24.0966796875,\n",
       " -30.692138671875,\n",
       " -28.5023193359375,\n",
       " -13.79547119140625,\n",
       " -29.8031005859375,\n",
       " -14.0010986328125,\n",
       " -26.9931640625,\n",
       " -20.8192138671875,\n",
       " -0.85589599609375,\n",
       " -23.7109375]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, data_train, data_test, metrics = tuner.get_file_in_directory()\n",
    "metrics['errors'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-24.09667969, -30.69213867, -28.50231934, -13.79547119,\n",
       "       -29.80310059, -14.00109863, -26.99316406, -20.81921387,\n",
       "        -0.855896  , -23.7109375 ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = tuner.get_prediction(model, data_test.scaled_x, data_test.scaler_y)\n",
    "(prediction - data_test.y).reshape(-1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "309491dbbc1eabbff0bd8e66af0a0c930ced039b058598366b1ba756b38781f0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('skripsi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
