{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from bs4 import Comment\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_scraping_website(website, full_data):\n",
    "    \"Nilai dari parameter website adalah bisnis.com dan detik finance\"\n",
    "    website_data = full_data[full_data.website == website]\n",
    "    last_scraping = (website_data.published_at.max() + pd.DateOffset(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    yesterday = (datetime.datetime.today() - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    return last_scraping, yesterday\n",
    "\n",
    "def generate_date_range(start, end):\n",
    "    \"\"\"Generate tanggal dengan rentang tertentu. Parameter\n",
    "    'Start' dan 'End' harus berformat Y-m-d. Misalnya 2001-01-27\n",
    "    \"\"\"\n",
    "    \n",
    "    start = datetime.datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end = datetime.datetime.strptime(end, \"%Y-%m-%d\") + datetime.timedelta(days=1)\n",
    "    return [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Scraping Bisnis.com\n",
    "\n",
    "### Scraping Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisnis_scrape_title_page(halaman, tanggal, category):\n",
    "    \"\"\"Melakukan request kehalaman list berita dari webiste bisnis.com \n",
    "    pada tanggal tertentu dengan halaman tertentu pada kategori tertentu\"\"\"\n",
    "    \n",
    "    URL = \"https://www.bisnis.com/index/page/?c={}&d={}&per_page={}\".format(category, tanggal, halaman)\n",
    "    page = requests.get(URL, headers={'user-agent': 'My app'})\n",
    "    return BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "def bisnis_extract_title_information(berita):\n",
    "    \"\"\"Mengambil informasi judul berita, link, dan waktu publish\"\"\"\n",
    "    \n",
    "    div = berita.find('div', class_=\"col-sm-8\")\n",
    "    a = div.select(\"h2 a\")[0]\n",
    "    return {\n",
    "        \"title\" : a['title'],\n",
    "        \"category\": div.select(\".wrapper-description a\")[0]['title'],\n",
    "        \"published_at\" : div.select(\".date\")[0].text.strip(),\n",
    "        \"link\": a['href']\n",
    "    }\n",
    "\n",
    "def bisnis_scrape_title_tanggal(tanggal, category):\n",
    "    \"\"\"Scraping judul pada tanggal tertentu\"\"\"\n",
    "    \n",
    "    halaman_sekarang = 1\n",
    "    beritas = []\n",
    "\n",
    "    # Dalam satu hari bisa terdiri dari banyak berita\n",
    "    # sehingga bisa terdiri dari banyak halaman\n",
    "    while True:\n",
    "        soup = bisnis_scrape_title_page(halaman_sekarang, tanggal, category)\n",
    "        list_berita = soup.find(\"ul\", class_=\"list-news\").find_all(\"li\")\n",
    "        \n",
    "        # Jika list berita kosong maka sudah sampai dihalaman terakhir\n",
    "        if (len(list_berita) == 1) and (list_berita[0].find(\"h2\").text.strip() == \"Tidak ada berita\"):\n",
    "            break\n",
    "            \n",
    "        for berita in list_berita:\n",
    "            beritas.append(bisnis_extract_title_information(berita))\n",
    "\n",
    "        halaman_sekarang += 1\n",
    "        \n",
    "    return beritas\n",
    "\n",
    "\n",
    "def bisnis_scrape_title(start, end, category):\n",
    "    \"\"\"Scraping untuk rentang tertentu. Hasilnya akan disimpan pada folder data\n",
    "    masih dalam bentuk json karena akan dilakukan penyimpanan setiap 150 hari\n",
    "    \"\"\"\n",
    "    \n",
    "    generated_date = generate_date_range(start, end)\n",
    "    hasil = []\n",
    "    status = []\n",
    "    website = f\"bisnis.com, {'Market' if category == 194 else 'Financial'}\"\n",
    "    print(f\"Start Scraping {website} from {start} until {end}\")\n",
    "    \n",
    "    iterate = 1\n",
    "    for date in generated_date:\n",
    "        tanggal_ymd = date.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        print(f\"Start Scraping title in {website} category on {tanggal_ymd}\")\n",
    "        jumlah_berita = 0\n",
    "        \n",
    "        try:\n",
    "            list_berita = bisnis_scrape_title_tanggal(tanggal_ymd, category)            \n",
    "            status_scrape = \"success\"\n",
    "            jumlah_berita = len(list_berita)\n",
    "            \n",
    "            hasil.append({ \"date\" : tanggal_ymd, \"article_count\": jumlah_berita, \"data\" : list_berita})\n",
    "            \n",
    "            status.append({ \"date\" : tanggal_ymd, \"status\" : status_scrape, \"article_count\": jumlah_berita,\n",
    "                           \"website\" : website, \"message\" : None})\n",
    "            \n",
    "        except Exception as e:\n",
    "            status_scrape = \"failed\"\n",
    "            status.append({ \"date\" : tanggal_ymd, \"status\" : status_scrape, \"article_count\": None,\n",
    "                            \"website\" : website, \"message\" : str(e)})\n",
    "        \n",
    "        print(f\"Scraping {tanggal_ymd} {status_scrape}, numbers of article is {jumlah_berita}\\n\")\n",
    "        \n",
    "    hasil = pd.DataFrame(hasil)\n",
    "    \n",
    "    # memisahkan kolom data menjadi kolom-kolom terpisah\n",
    "    data_full = None\n",
    "    for index, row in hasil.iterrows():\n",
    "        data_satu_hari = pd.DataFrame(row['data'])\n",
    "        data_satu_hari['date'] = row['date']\n",
    "\n",
    "        if data_full is None: data_full = pd.DataFrame(data_satu_hari)\n",
    "        else: data_full = pd.concat([data_full, pd.DataFrame(data_satu_hari)])\n",
    "\n",
    "    data_full['published_at'] = pd.to_datetime(data_full.published_at, format=\"%d %b %Y | %H:%M WIB\")\n",
    "    return status, data_full.sort_values(\"published_at\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_artikel_bisnis(text, stop_words):\n",
    "    \"\"\" Untuk melakukan preprocessing sederhana hasil scraping\n",
    "    seperti menghapus line break, whitespace dan kata-kata yang\n",
    "    tidak bermakna yang sering muncul diawal atau akhir artikel\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return text\n",
    "    \n",
    "    for sentance in stop_words:\n",
    "        text = text.replace(sentance, \"\")\n",
    "\n",
    "    # hapus whitespace diawal dan akhir artikel dan hapus semua line break\n",
    "    text = text.strip().replace('\\n', ' ').replace('\\r', '')\n",
    "    \n",
    "    # hapus strip diawalan artikel\n",
    "    text =  re.sub(\"^(-+)*?\", \"\", text).strip()\n",
    "    \n",
    "    # jika awalannya seperti ini, hapus aja datanya, ga ada informasinya\n",
    "    if text[:30] == \"Simak berita lainnya seputar t\":\n",
    "        return None\n",
    "    \n",
    "    return text\n",
    "\n",
    "def scraping_one_article_bisnis(data_title, stop_words):\n",
    "    \"\"\"Scraping satu halaman tertentu dengan menggunakan\n",
    "    data title yang sudah di scraping sebelumnya\n",
    "    \"\"\"\n",
    "    iterasi = 0\n",
    "    status_error = []\n",
    "    data_per_tanggal = []\n",
    "\n",
    "    for tanggal, df, in data_title.groupby(\"date\"):\n",
    "        print(f\"Scraping content article of bisnis.com on {tanggal}\")\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            tmp = {'title' : row['title'], 'link' : row['link'], 'date' : tanggal}\n",
    "\n",
    "            try:\n",
    "                page = requests.get(tmp['link'], headers={'user-agent': 'My app'})\n",
    "                page = BeautifulSoup(page.content, 'html.parser')\n",
    "                \n",
    "                # Di bisnis.com ada beberapa artikel yang tidak dapat diakses secara\n",
    "                # umum dan harus berbayar\n",
    "                if len(page.find_all(\"div\", class_=\"wrapper-premium-content\")) > 0:\n",
    "                        raise Exception(\"premium content\")\n",
    "                        \n",
    "                # 2 paragraf terakhir dibuang karena hanya mengandung iklan\n",
    "                article_body = page.select(\".description .sticky-wrapper .col-sm-10\")[0]\n",
    "                paragraphs = article_body.find_all(\"p\")[:-2]\n",
    "                \n",
    "                # setiap paragraf dibelakangnya akan ditambahkan titik dan spasi\n",
    "                isi_artikel = \" \".join([paragraph.text.strip().rstrip('.') + '. ' for paragraph in paragraphs])    \n",
    "                tmp['content'] = clean_artikel_bisnis(isi_artikel, stop_words)\n",
    "                data_per_tanggal.append(tmp)\n",
    "\n",
    "            except Exception as error:\n",
    "                status_error.append({'date' : tanggal, \"website\" : \"bisnis.com\",\n",
    "                                     'link': tmp['link'], 'message' : str(error)})\n",
    "                time.sleep(2)\n",
    "\n",
    "            iterasi += 1\n",
    "            # setiap 200 iterasi, istirahat 15 detik.\n",
    "            if iterasi % 200 == 0:\n",
    "                time.sleep(15)\n",
    "                \n",
    "    # jika status errornya kosong maka buat dataframe kosong\n",
    "    column_names = [\"date\", \"website\", \"link\", \"message\"]\n",
    "    status_error = pd.DataFrame(status_error) if len(status_error) > 0 else pd.DataFrame(columns = column_names)\n",
    "    return status_error, pd.DataFrame(data_per_tanggal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Scraping Detik Finance\n",
    "\n",
    "### Scraping Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detik_scrape_title_page(halaman, tanggal):\n",
    "    \"\"\"Melakukan request kehalaman list judul detik finance \n",
    "    pada tanggal tertentu dengan halaman tertentu\"\"\"\n",
    "    \n",
    "    URL = \"https://finance.detik.com/indeks/{}?date={}\".format(halaman, tanggal)\n",
    "    page = requests.get(URL)\n",
    "    return BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "def detik_extract_title_information(berita):\n",
    "    \"\"\"Mengambil informasi judul berita, link, dan waktu publish\"\"\"\n",
    "    \n",
    "    media_text = berita.find(\"div\", class_=\"media__text\")\n",
    "\n",
    "    media_title = media_text.find(\"h3\", class_=\"media__title\").find(\"a\")\n",
    "    title = media_title.text.strip()\n",
    "    link = media_title[\"href\"]\n",
    "    kategori = link.split(\"/\")[3].replace(\"-\", \" \")\n",
    "\n",
    "    media_date = media_text.find(\"div\", class_=\"media__date\").find(\"span\")[\"d-time\"]\n",
    "\n",
    "    return {\"title\" : title, \"category\" : kategori, \"published_at\" : media_date,\"link\" : link}\n",
    "\n",
    "def detik_scrape_title_tanggal(tanggal):\n",
    "    \"\"\"Scraping judul artikel pada tanggal tertentu\n",
    "    \"\"\"\n",
    "    \n",
    "    is_first = True\n",
    "    halaman_sekarang = 1\n",
    "    jumlah_halaman = 100\n",
    "    beritas = []\n",
    "\n",
    "    # Dalam satu tanggal bisa saja beritanya banyak sehingga dipecah menjadi\n",
    "    # beberapa halaman (pagination)\n",
    "    while halaman_sekarang <= jumlah_halaman:\n",
    "        soup = detik_scrape_title_page(halaman_sekarang, tanggal)\n",
    "\n",
    "        list_berita = soup.find_all(\"article\", class_=\"list-content__item\")    \n",
    "        if len(list_berita) > 0:\n",
    "            for berita in list_berita:\n",
    "                beritas.append(detik_extract_title_information(berita))\n",
    "\n",
    "            # Jika halaman pertama, cek ada berapa halaman dengan\n",
    "            # melihat pada jumlah pagination dibagian bawah halaman\n",
    "            # kemudian update jumlah halaman sehingga loop 'while' bisa berakhir\n",
    "            if is_first:\n",
    "                is_first = False\n",
    "                list_pagination = soup.find_all(\"a\", class_=\"pagination__item\")\n",
    "                idx_last_page = len(list_pagination) - 2\n",
    "                jumlah_halaman = int(list_pagination[idx_last_page].text)\n",
    "        elif is_first:\n",
    "            jumlah_halaman = 0\n",
    "\n",
    "        halaman_sekarang += 1\n",
    "        \n",
    "    return beritas\n",
    "\n",
    "def detik_scrape_title(start, end):\n",
    "    \"\"\"Scraping judul artikel untuk rentang tertentu.\n",
    "    \"\"\"\n",
    "    \n",
    "    generated_date = generate_date_range(start, end)\n",
    "    hasil = []\n",
    "    status = []    \n",
    "    iterate = 1\n",
    "    print(f\"Start Scraping detik finance from {start} until {end}\")\n",
    "    \n",
    "    for date in generated_date:\n",
    "        tanggal_ymd = date.strftime(\"%Y-%m-%d\")\n",
    "        tanggal_mdy = date.strftime(\"%m/%d/%Y\") # format url di detik finance m-d-y\n",
    "        \n",
    "        print(f\"Start Scraping title in detik finance website on {tanggal_ymd}\")\n",
    "        jumlah_berita = 0\n",
    "        try:\n",
    "            list_berita= detik_scrape_title_tanggal(tanggal_mdy)\n",
    "            status_scrape = \"success\"\n",
    "            jumlah_berita = len(list_berita)\n",
    "\n",
    "            hasil.append({ \"date\" : tanggal_ymd, \"article_count\": jumlah_berita, \"data\" : list_berita})\n",
    "            status.append({ \"date\" : tanggal_ymd, \"status\" : status_scrape, \"article_count\": jumlah_berita,\n",
    "                           \"website\" : \"detik finance\", \"message\" : None})\n",
    "            \n",
    "        except Exception as e:  \n",
    "            status_scrape = \"failed\"\n",
    "            status.append({ \"date\" : tanggal_ymd, \"status\" : status_scrape, \"jumlah_berita\": None, \n",
    "                           \"website\" : \"detik finance\", \"message\" : str(e)})\n",
    "\n",
    "        print(f\"Scraping {tanggal_ymd} {status_scrape}, numbers of article is {jumlah_berita}\\n\")\n",
    "        \n",
    "    hasil = pd.DataFrame(hasil)\n",
    "    \n",
    "    # pisahkan kolom data menjadi beberapa kolom\n",
    "    data_full = None\n",
    "    for index, row in hasil.iterrows():\n",
    "        data_satu_hari = pd.DataFrame(row['data'])\n",
    "        data_satu_hari['date'] = row['date']\n",
    "\n",
    "        if data_full is None: data_full = pd.DataFrame(data_satu_hari)\n",
    "        else: data_full = pd.concat([data_full, pd.DataFrame(data_satu_hari)])\n",
    "\n",
    "    data_full = data_full.reset_index(drop=True)\n",
    "    # data waktu yang di scraping merupakan waktu dalam bentuk timestamp sehingga perlu\n",
    "    # dikonversi terlebih dahulu dan ditambah 7 jam agar mengikuti waktu WIB\n",
    "    data_full[\"published_at\"] = pd.to_datetime(data_full['published_at'], unit='s') + pd.DateOffset(hours=7)\n",
    "\n",
    "    return pd.DataFrame(status), data_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_artikel_detik(text):\n",
    "    \"\"\" Untuk melakukan preprocessing sederhana hasil scraping\n",
    "    seperti menghapus line break, whitespace dan kata-kata yang\n",
    "    tidak bermakna yang sering muncul diawal atau akhir artikel\n",
    "    \"\"\"\n",
    "    \n",
    "    if text is None:\n",
    "        return text\n",
    "    \n",
    "    # hapus line break\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '').replace('\\t', ' ').strip()\n",
    "    \n",
    "    # identifikasi tempat penulisan artikel dan hapus tulisannya\n",
    "    tmp = text.split(\"-\")\n",
    "    if (len(tmp) > 1) and (len(tmp[0]) < 27):\n",
    "        awalan = tmp[0] + \"-\"\n",
    "        text = text.replace(awalan, \"\").strip()\n",
    "        \n",
    "    # Menghapus kata-kata yang tidak perlu\n",
    "    sentances = [\n",
    "        \"Lanjut ke halaman berikutnya >>>\",\n",
    "        \"berikut ini:\",\n",
    "        \"Berikut daftar lengkapnya\",\n",
    "        \"[Gambas:Video 20detik]\",\n",
    "        \"http://bit.ly/18anniversary.\",\n",
    "        \"Berikut informasi selengkapnya.\",\n",
    "        \"redaksi@detikFinance.com.\",\n",
    "        \"Berikut informasi selengkapnya:\",\n",
    "        \"http://bit.ly/promoakhirpekan.\",\n",
    "        \"Semoga bermanfaat\"\n",
    "    ]\n",
    "    \n",
    "    for sentance in sentances:\n",
    "        text = text.replace(sentance, \"\")\n",
    "    \n",
    "    # List kata-kata yang sering muncul diakhir artikel\n",
    "    text = re.sub('(Lihat juga Video: .+)$', \"\", text)\n",
    "    text = re.sub('(Simak Video .+)$', \"\", text)\n",
    "    text = re.sub('(Disclaimer: .+)$', \"\", text)\n",
    "    text = re.sub('(Mau tahu informasi selengkapnya\\? .+)$', \"\", text)\n",
    "    text = re.sub('(Berikut berita selengkapnya .+)$', \"\", text)\n",
    "    text = re.sub('(Untuk mengetahui informasi lebih lengkap, .+)$', \"\", text)\n",
    "    text = re.sub('(Untuk informasi lebih lengkap, .+)$', \"\", text)\n",
    "    text = re.sub('(Khusus Untuk 100 Orang Pembaca .+)$', \"\", text)\n",
    "    text = re.sub('(Mau Menjadi Kaya Atau Bahkan Lebih Kaya .+)$', \"\", text)\n",
    "    text = re.sub('(Untuk mengetahui promo lain yang berlangsung .+)$', \"\", text)\n",
    "    text = re.sub('(Untuk promo lebih lengkap, .+)$', \"\", text)\n",
    "    text = re.sub('(Untuk melihat promo lainnya, .+)$', \"\", text)\n",
    "    text = re.sub('(Untuk mengetahui promo lainnya, .+)$', \"\", text)\n",
    "    text = re.sub('(Untuk mengetahui promo .+)$', \"\", text)\n",
    "    text = re.sub('(Dapatkan eBook Tentang \"61 Rahasia .+)$', \"\", text)\n",
    "    text = re.sub('(Sudah Siapkah Anda di Tahun 2018\\? .+)$', \"\", text)\n",
    "    text = re.sub('(Sudah Siapkah Anda di Tahun 2018 ini\\? .+)$', \"\", text)\n",
    "    text = re.sub('(Sudah Siapkah Anda Ditahun 2018\\? .+)$', \"\", text)\n",
    "    text = re.sub('(Saksikan juga video .+)$', \"\", text)\n",
    "    text = re.sub('(Lihat Video: .+)$', \"\", text)\n",
    "    text = re.sub('(Baca 5 berita .+)$', \"\", text)\n",
    "    text = re.sub('(Bisa kirim ceritanya ke .+)$', \"\", text)\n",
    "    text = re.sub('(Tonton video .+)$', \"\", text)\n",
    "    text = re.sub('(Tonton juga Video: .+)$', \"\", text)    \n",
    "    text = re.sub('(Untuk mengetahui informasi dari .+)$', \"\", text)\n",
    "    text = re.sub('(Hasil Intisari Pemikiran Dan Praktek Saya .+)$', \"\", text)\n",
    "    text = re.sub('(Dapatkan eBook Tentang .+)$', \"\", text)\n",
    "    text = re.sub('(Mau Ikut Pelatihan dengan Materi Terbaru .+)$', \"\", text)\n",
    "    text = re.sub('(Mau ikut diskusi soal Aturan Baru Taksi .+)$', \"\", text)\n",
    "    text = re.sub('(Bagaimana Kaya Secara Aman di Tahun 2017 ke 2018 ini\\? .+)$', \"\", text)\n",
    "    text = re.sub('(Untuk mengetahui info lengkap .+)$', \"\", text)\n",
    "    text = re.sub('(Mau eBook Tentang .+)$', \"\", text)\n",
    "    text = re.sub('(Mau Menjadi Kaya Atau Bahkan Lebih Kaya\\? .+)$', \"\", text)\n",
    "    text = re.sub('(Ikuti terus berita tentang .+)$', \"\", text)\n",
    "    text = re.sub('(Untuk melihat promo .+)$', \"\", text)\n",
    "    text = re.sub('(GRATIS\\. .+)$', \"\", text)\n",
    "    text = re.sub('(Tonton juga .+)$', \"\", text)\n",
    "    text = re.sub('(Hasil Intisari Pemikiran Dan Praktik Saya .+)$', \"\", text)\n",
    "    text = re.sub('(Simak berita selengkapnya .+)$', \"\", text)\n",
    "    text = re.sub('(Untuk mengetahui info .+)$', \"\", text)\n",
    "    text = re.sub(\"(Siap 'Bantu Jokowi Cari Menteri'\\? .+)$\", \"\", text)\n",
    "    text = re.sub(\"(bagi detikers yang .+)$\", \"\", text)\n",
    "    text = re.sub(\"(Ingin tahu kisah lain .+)$\", \"\", text)\n",
    "    text = re.sub(\"(Untuk ilmu yang lebih lengkap lagi, .+)$\", \"\", text)\n",
    "    text = re.sub(\"(Untuk mengetahui beragam promo .+)$\", \"\", text)\n",
    "    text = re.sub(\"(Simak ulasan lengkapnya .+)$\", \"\", text)\n",
    "    text = re.sub(\"(Di Jakarta dibuka workshop sehari tentang bagaimana .+)$\", \"\", text)\n",
    "    text = re.sub(\"(detikcom bersama BRI .+)$\", \"\", text)\n",
    "    text = re.sub(\"(detikcom bersama Bank BRI .+)$\", \"\", text)\n",
    "    text = re.sub(\"(Promo ini masih ditambah .+)$\", \"\", text)\n",
    "    text = re.sub(\"(Berbagai promo ini masih ditambah .+)$\", \"\", text)\n",
    "    text = re.sub(\"(Program ini untuk melengkapi promo .+)$\", \"\", text)\n",
    "    text = re.sub(\"(bagi yang bertransaksi .+)$\", \"\", text)\n",
    "    text = re.sub(\"(Untuk transaksi pembelian emas batangan .+)$\", \"\", text)\n",
    "    text = re.sub(\"(Untuk transaksi pembelian Emas .+)$\", \"\", text)\n",
    "    text = re.sub(\"(Dapatkan berbagai promo .+)$\", \"\", text)\n",
    "    text = re.sub(\"(Baca berita lainnya .+)$\", \"\", text)\n",
    "    \n",
    "    return text.strip()\n",
    "    \n",
    "def scraping_one_article_detik(data_judul):\n",
    "    \"\"\"Scraping satu halaman tertentu dengan menggunakan\n",
    "    data title yang sudah di scraping sebelumnya\n",
    "    \"\"\"\n",
    "    \n",
    "    iterasi = 0\n",
    "    status_error = []\n",
    "    data_per_tanggal = []\n",
    "\n",
    "    for tanggal, df, in data_judul.groupby(\"date\"):\n",
    "        print(f\"Scraping content article in detik finance website on {tanggal}\")\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            tmp = {'title' : row['title'], 'link' : row['link'], 'date' : tanggal}            \n",
    "\n",
    "            try:\n",
    "                URL = f\"{tmp['link']}?single=1\"\n",
    "                page = requests.get(URL, headers={'user-agent': 'My app'})\n",
    "                page = BeautifulSoup(page.content, 'html.parser')\n",
    "                body_content = page.select(\".detail__body-text\")[0]\n",
    "                \n",
    "                # Hapus tags tertentu yang tidak mengandung makna\n",
    "                remove_tags = [\"div\", \"table\", \"style\", \"script\", \"strong\"]\n",
    "                for tag in remove_tags:\n",
    "                    for html_tag in body_content.find_all(tag):\n",
    "                        html_tag.decompose()\n",
    "                \n",
    "                # Hapus komentar html\n",
    "                for child in body_content.children:\n",
    "                    if isinstance(child,Comment):\n",
    "                        child.extract()\n",
    "                        \n",
    "                paragraphs = body_content.find_all(\"p\")\n",
    "                if len(paragraphs) == 0:\n",
    "                    raise Exception(\"no paragraph\")\n",
    "                \n",
    "                # setiap paragraf akan dicek apakah ada isinya atau tidak\n",
    "                # jika ada maka diakhir akan ditambahi titik dan spasi\n",
    "                isi_artikel = []\n",
    "                for paragraph in paragraphs:\n",
    "                    text = paragraph.text.strip().rstrip('.')\n",
    "                    if len(text) > 0:\n",
    "                        text = text + '. ' \n",
    "                        isi_artikel.append(text)\n",
    "\n",
    "                isi_artikel = \" \".join(isi_artikel).strip()\n",
    "                # jika artikelnya ternyata kosong, throw exception\n",
    "                if len(isi_artikel) == 0:\n",
    "                    raise Exception(\"no content\")\n",
    "\n",
    "                tmp['content'] = clean_artikel_detik(isi_artikel)\n",
    "            except Exception as error:\n",
    "                status_error.append({\"date\" : tanggal, \"website\" : \"detik finance\", \n",
    "                                     'link': tmp['link'], 'message' : str(error)})\n",
    "                time.sleep(3)\n",
    "\n",
    "            data_per_tanggal.append(tmp)\n",
    "            iterasi += 1\n",
    "\n",
    "            # setiap iterasi ke 200 istirahat 15 detik\n",
    "            if iterasi % 200 == 0:\n",
    "                time.sleep(10)\n",
    "    \n",
    "    # jika status errornya kosong maka buat dataframe kosong\n",
    "    column_names = [\"date\", \"website\", \"link\", \"message\"]\n",
    "    status_error = pd.DataFrame(status_error) if len(status_error) > 0 else pd.DataFrame(columns = column_names)\n",
    "    return status_error, pd.DataFrame(data_per_tanggal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_bisnis(article_related_to_saham):\n",
    "    \"\"\"Fungsi ini akan melakukan scraping pada \n",
    "    website bisnis.com kategori market dan financial\n",
    "    dengan mengambil data dari tanggal terakhir data yang ada\n",
    "    pada data article_related_to_saham.json sampai dengan kemarin\n",
    "    \"\"\"\n",
    "    \n",
    "    ordered_columns = [\"title\", \"content\", \"category\", \"published_at\", \"link\", \"website\"]\n",
    "    bisnis_last_scraping, yesterday = last_scraping_website(\"bisnis.com\", article_related_to_saham)\n",
    "\n",
    "    # c = 194 adalah kategori artikel market di bisnis.com\n",
    "    # c = 5 adalah kategori artikel financial di bisnis.com\n",
    "    bisnis_market_status, bisnis_market_title = bisnis_scrape_title(bisnis_last_scraping, yesterday, category=194)\n",
    "    bisnis_financial_status, bisnis_financial_title = bisnis_scrape_title(bisnis_last_scraping, yesterday, category=5)\n",
    "\n",
    "    # Satukan data status scraping antara kategori market dan financial\n",
    "    bisnis_status_scraping_title = pd.DataFrame(bisnis_market_status + bisnis_financial_status)\n",
    "\n",
    "    # Satukan data title antara kategori market dan financial\n",
    "    bisnis_article_title = (pd.concat([bisnis_market_title, bisnis_financial_title])\n",
    "                            .sort_values(\"published_at\")\n",
    "                            .reset_index(drop=True))\n",
    "\n",
    "    # Scraping full artikel\n",
    "    bisnis_stop_words = pd.read_excel(\"data/stop_words_bisnis.xlsx\").text.to_list()\n",
    "    bisnis_status_error, bisnis_article = scraping_one_article_bisnis(bisnis_article_title, bisnis_stop_words)\n",
    "    bisnis_article['website'] = \"bisnis.com\"\n",
    "\n",
    "    # Ambil data category dengan published at pada tabel title\n",
    "    bisnis_article = (bisnis_article\n",
    "                      .set_index('link')\n",
    "                      .join(bisnis_article_title\n",
    "                            .set_index('link')[['category', 'published_at']])\n",
    "                      .reset_index()[ordered_columns])\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    return bisnis_article, bisnis_status_scraping_title, bisnis_status_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_detik(article_related_to_saham):\n",
    "    \"\"\"Fungsi ini akan melakukan scraping pada \n",
    "    website detik finance dengan mengambil data dari \n",
    "    tanggal terakhir data yang ada pada data \n",
    "    article_related_to_saham.json sampai dengan kemarin\n",
    "    \"\"\"\n",
    "    \n",
    "    ordered_columns = [\"title\", \"content\", \"category\", \"published_at\", \"link\", \"website\"]\n",
    "    detik_last_scraping, yesterday = last_scraping_website(\"detik finance\", article_related_to_saham)\n",
    "\n",
    "    # scraping title\n",
    "    detik_status_scraping_title, detik_article_title = detik_scrape_title(detik_last_scraping, yesterday)\n",
    "\n",
    "    # scraping content\n",
    "    detik_status_error, detik_article = scraping_one_article_detik(detik_article_title)\n",
    "    detik_article['website'] = \"detik finance\"\n",
    "\n",
    "    detik_article = (detik_article\n",
    "                     .set_index('link')\n",
    "                     .join(detik_article_title\n",
    "                           .set_index('link')[['category', 'published_at']])\n",
    "                     .reset_index()[ordered_columns])\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    return detik_article, detik_status_scraping_title, detik_status_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_artikel_saham(row, all_saham):\n",
    "    \"\"\"Fungsi ini adalah fungsi callback\n",
    "    yang digunakan untuk melihat apakah dalam satu artikel\n",
    "    ada disebutkan nama saham atau nama perusahaan pemilik saham\n",
    "    sehingga bisa diketahui artikel mana yang membahas saham\n",
    "    dan mana yang tidak\n",
    "    \"\"\"\n",
    "    title = row['title']\n",
    "    content = str(row['content'])\n",
    "    artikel_saham = []\n",
    "    \n",
    "    if content is not None:\n",
    "        content_lower = content.lower()\n",
    "\n",
    "        def contains_word(sentance, word):\n",
    "            return (' ' + word + ' ') in (' ' + sentance + ' ')\n",
    "\n",
    "        # untuk satu artikel akan dilakukan pengecekan untuk seluruh saham\n",
    "        for index, saham in all_saham.iterrows():\n",
    "            get_artikel = False\n",
    "            \n",
    "            kode_saham = saham['code']\n",
    "            nama_perusahaan = saham['company_name']\n",
    "\n",
    "            tmp = {\"code\" : kode_saham, \"company_name\" : nama_perusahaan, \"link\" : row['link']}\n",
    "\n",
    "            # cek apakah content artikel mengandung kode saham\n",
    "            # biasanya kode saham akan ditaruh di dalam kurung\n",
    "            if contains_word(content, f\"({kode_saham})\"):\n",
    "                tmp['keyword'] = kode_saham\n",
    "                artikel_saham.append(tmp)\n",
    "            # cek apakah artikel mengandung nama perusahaan tertentu\n",
    "            elif contains_word(content, nama_perusahaan):\n",
    "                tmp['keyword'] = nama_perusahaan\n",
    "                artikel_saham.append(tmp)\n",
    "            else:\n",
    "                # jika kedua kondisi sebelumnya tidak terpenuhi maka akan dicek\n",
    "                # apakah content yang telah dilower case mengandung keyword\n",
    "                # yang telah ditentukan sebelumnya\n",
    "                keywords = [] if saham['keyword'] is None else str(saham['keyword']).split(';')\n",
    "\n",
    "                for keyword in keywords:\n",
    "                    if contains_word(content_lower, keyword):\n",
    "                        tmp['keyword'] = keyword\n",
    "                        artikel_saham.append(tmp)\n",
    "                        break                        \n",
    "    \n",
    "    # jika tidak ditemukan maka isikan kolom baru dengan None bukan \n",
    "    # dengan array kosong sehingga nanti mudah untuk diidentifikasi menggunakan fungsi isnull\n",
    "    row['related_to_saham'] = artikel_saham if len(artikel_saham) > 0 else None\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_data():\n",
    "    \"\"\"Fungsi ini akan melakukan backup data\n",
    "    kedalam folder backup. Setiap file hasil backup\n",
    "    akan ditambahkan prefix timestamp\n",
    "    \"\"\"\n",
    "    if not os.path.exists('data/backup'):\n",
    "        os.makedirs('data/backup')\n",
    "\n",
    "    print(\"Melakukan Backup Data\")\n",
    "    now = int(datetime.datetime.now().timestamp())\n",
    "    shutil.copyfile(\"data/article_related_to_saham.json\", f\"data/backup/{now}_article_related_to_saham.json\")\n",
    "    shutil.copyfile(\"data/article_norelated_to_saham.json\", f\"data/backup/{now}_article_norelated_to_saham.json\")\n",
    "    shutil.copyfile(\"data/table_articles.json\", f\"data/backup/{now}_table_articles.json\")\n",
    "    shutil.copyfile(\"data/table_article_saham.json\", f\"data/backup/{now}_table_article_saham.json\")\n",
    "    \n",
    "    if os.path.exists(\"data/status_scraping_title.xlsx\"):\n",
    "        shutil.copyfile(\"data/status_scraping_title.xlsx\", f\"data/backup/{now}_status_scraping_title.xlsx\")\n",
    "        \n",
    "    if os.path.exists(\"data/status_error.xlsx\"):\n",
    "        shutil.copyfile(\"data/status_error.xlsx\", f\"data/backup/{now}_status_error.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status_scraping_title(status_scraping_title):\n",
    "    \"\"\"Fungsi ini akan melakukan update file\n",
    "    excel status_scraping_title yang menyimpan informasi\n",
    "    apakah scraping title pada tanggal tertentu gagal atau tidak\n",
    "    \"\"\"\n",
    "    \n",
    "    # cek apakah filenya sudah ada atau belum. \n",
    "    # Jika ada read file tersebut akan tetapi \n",
    "    # jika tidak maka buat data frame kosong yang baru\n",
    "    if os.path.exists(\"data/status_scraping_title.xlsx\"):\n",
    "        status_scraping_title_old = pd.read_excel(\"data/status_scraping_title.xlsx\")\n",
    "    else:\n",
    "        status_scraping_title_old = pd.DataFrame(columns=[\"date\", \"status\", \"article_count\", \"website\", \"message\"])\n",
    "\n",
    "    (pd.concat([status_scraping_title_old, status_scraping_title])\n",
    "     .reset_index(drop=True).to_excel(\"data/status_scraping_title.xlsx\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status_error(status_error):\n",
    "    \"\"\"Fungsi ini akan melalukan update file\n",
    "    status_error yang mana merupakan file yang akan \n",
    "    menyimpan data link yang gagal dilakukan scraping beserta alasannya\n",
    "    \"\"\"\n",
    "    \n",
    "    # cek apakah filenya sudah ada atau belum. \n",
    "    # Jika ada read file tersebut akan tetapi \n",
    "    # jika tidak maka buat data frame kosong yang baru\n",
    "    if os.path.exists(\"data/status_error.xlsx\"):\n",
    "        status_error_old = pd.read_excel(\"data/status_error.xlsx\")\n",
    "    else:\n",
    "        status_error_old = pd.DataFrame(columns=[\"date\", \"website\", \"link\", \"message\"])\n",
    "\n",
    "    (pd.concat([status_error_old, status_error])\n",
    "     .reset_index(drop=True).to_excel(\"data/status_error.xlsx\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_saham(start=\"01-01-2000 08\"):\n",
    "    \"\"\"Fungsi ini untuk melakukan update\n",
    "    pada data saham\n",
    "    \"\"\"\n",
    "    start = datetime.datetime.strptime(start, \"%d-%m-%Y %H\")\n",
    "    start = int(datetime.datetime.timestamp(start))\n",
    "    \n",
    "    end = datetime.datetime.now()\n",
    "    end = end.strftime(\"%d-%m-%Y\") + \" 08\"\n",
    "    end = datetime.datetime.strptime(end, \"%d-%m-%Y %H\") - datetime.timedelta(days=1)\n",
    "    end = int(datetime.datetime.timestamp(end))\n",
    "    \n",
    "    if not os.path.exists('data/harga_saham'):\n",
    "        os.makedirs('data/harga_saham')\n",
    "    \n",
    "    list_saham = [\"BBNI\", \"BBRI\", \"BMRI\", \"BBCA\"]  \n",
    "    for saham in list_saham:\n",
    "        link = f\"https://query1.finance.yahoo.com/v7/finance/download/{saham}.JK?period1={start}&period2={end}&interval=1d&events=history&includeAdjustedClose=true\"\n",
    "        page = requests.get(link, headers={'user-agent': 'My app'})\n",
    "        open(f'data/harga_saham/{saham}.csv', 'wb').write(page.content)\n",
    "    print(\"Saham data has been updated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data():\n",
    "    \"\"\"Fungsi ini digunakan untuk melakukan pengecekan sederhana\n",
    "    setelah data berhasil diupdate. Hanya melakukan pengecekan\n",
    "    ke konsistenan data\n",
    "    \"\"\"\n",
    "    tb_articles = pd.read_json('data/table_articles.json')\n",
    "    tb_articles_saham = pd.read_json('data/table_article_saham.json')\n",
    "    has_error = False\n",
    "    \n",
    "    if tb_articles.id.max() != tb_articles.shape[0]:\n",
    "        print(\"Id Article tidak cocok\")\n",
    "        has_error = True\n",
    "        \n",
    "    if tb_articles.link.duplicated().sum() != 0:\n",
    "        print(\"Terdapat duplikasi pada tabel article\")\n",
    "        has_error = True\n",
    "        \n",
    "    if tb_articles_saham[['saham_id', 'article_id']].duplicated().sum() != 0:\n",
    "        print(\"Terdapat duplikasi pada tabel article saham\")\n",
    "        has_error = True\n",
    "    \n",
    "    if not has_error:\n",
    "        print(\"Update data aman. Mantaaap!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Scraping bisnis.com, Market from 2021-12-31 until 2021-12-31\n",
      "Start Scraping title in bisnis.com, Market category on 2021-12-31\n",
      "Scraping 2021-12-31 success, numbers of article is 29\n",
      "\n",
      "Start Scraping bisnis.com, Financial from 2021-12-31 until 2021-12-31\n",
      "Start Scraping title in bisnis.com, Financial category on 2021-12-31\n",
      "Scraping 2021-12-31 success, numbers of article is 29\n",
      "\n",
      "Scraping content article of bisnis.com on 2021-12-31\n",
      "\n",
      "\n",
      "Start Scraping detik finance from 2021-12-31 until 2021-12-31\n",
      "Start Scraping title in detik finance website on 2021-12-31\n",
      "Scraping 2021-12-31 success, numbers of article is 96\n",
      "\n",
      "Scraping content article in detik finance website on 2021-12-31\n",
      "\n",
      "\n",
      "Melakukan Backup Data\n",
      "Data has been updated!\n",
      "Saham data has been updated!\n",
      "Update data aman. Mantaaap!!!\n"
     ]
    }
   ],
   "source": [
    "article_related_to_saham = pd.read_json(\"data/article_related_to_saham.json\")\n",
    "\n",
    "bisnis_article, bisnis_status_scraping_title, bisnis_status_error = scraping_bisnis(article_related_to_saham)\n",
    "detik_article, detik_status_scraping_title, detik_status_error = scraping_detik(article_related_to_saham)\n",
    "\n",
    "# Gabung antara artikel di detik dan bisnis.com\n",
    "article = pd.concat([detik_article, bisnis_article]).sort_values('published_at').reset_index(drop=True)\n",
    "\n",
    "# Gabung status scraping title\n",
    "status_scraping_title = (pd.concat([detik_status_scraping_title, bisnis_status_scraping_title])\n",
    "                         .sort_values(['date', 'website'])\n",
    "                         .reset_index(drop=True))\n",
    "\n",
    "# Gabung status error scraping artikel\n",
    "status_error = (pd.concat([detik_status_error, bisnis_status_error])\n",
    "                .sort_values(['date', 'website'])\n",
    "                .reset_index(drop=True))\n",
    "\n",
    "# Indentifikasi artikel mana saja yang berkaitan dengan saham atau perusahaan pemilik saham\n",
    "all_saham = pd.read_csv(\"data/daftar_saham.csv\")\n",
    "article = article.apply(lambda row : find_artikel_saham(row, all_saham), axis=1)\n",
    "\n",
    "# Pisahkan artikel yang tidak berkaitan dengan saham\n",
    "article_norelated_to_saham_new = (article[article.related_to_saham.isnull()]\n",
    "                       .drop('related_to_saham', axis=1)\n",
    "                       .reset_index(drop=True))\n",
    "\n",
    "# Pisahkan artikel yang berkaitan dengan saham\n",
    "article_related_to_saham_new = (article[article.related_to_saham.notnull()]\n",
    "                       .reset_index(drop=True))\n",
    "\n",
    "# ambil informasi yang ada di kolom related to saham\n",
    "related_to_saham = []\n",
    "for index, row in article_related_to_saham_new.iterrows():\n",
    "    related_to_saham = related_to_saham + row['related_to_saham']\n",
    "related_to_saham = pd.DataFrame(related_to_saham)\n",
    "\n",
    "# ambil hanya artikel yang berkaitan dengan bank yang telah ditentukan\n",
    "# yang ada pada data table_saham.json\n",
    "table_saham = pd.read_json('data/table_saham.json')\n",
    "selected_bank = table_saham.company_name.to_list()\n",
    "selected_artikel_bank = related_to_saham[related_to_saham.company_name.isin(selected_bank)]\n",
    "\n",
    "# ambil nilai id maksimal dari artikel yang sekarang sehingga \n",
    "# nilai id akan terus berlanjut\n",
    "table_article = pd.read_json(\"data/table_articles.json\")\n",
    "max_id = table_article.id.max()\n",
    "\n",
    "# artikel baru yang akan dimasukkan ke tabel article\n",
    "table_article_new = (article_related_to_saham_new\n",
    "                     .loc[article_related_to_saham_new.link.isin(selected_artikel_bank.link.to_list()), :]\n",
    "                     .reset_index(drop=True)\n",
    "                     .reset_index()\n",
    "                     .rename(columns={\"index\" : \"id\"}))\n",
    "table_article_new['id'] = table_article_new['id'] + 1 + max_id\n",
    "\n",
    "# article saham baru yang akan dimasukkan ke tabel article saham\n",
    "table_article_saham_new = (selected_artikel_bank\n",
    "                         .set_index(\"code\")\n",
    "                         .join(table_saham[['id', \"code\"]]\n",
    "                               .set_index('code'))\n",
    "                         .rename(columns={\"id\" : \"saham_id\"})\n",
    "                         .set_index(\"link\")\n",
    "                         .join(table_article_new[[\"id\", \"link\"]]\n",
    "                               .set_index(\"link\"))\n",
    "                         .rename(columns={\"id\" : \"article_id\"})\n",
    "                         .drop([\"company_name\"], axis=1)\n",
    "                         .reset_index(drop=True))\n",
    "\n",
    "# Gabung data antara data yang baru dengan \n",
    "# data yang lama yang sudah disimpan sebelumnya\n",
    "table_article = pd.concat([table_article, table_article_new]).reset_index(drop=True)\n",
    "\n",
    "table_article_saham = pd.read_json(\"data/table_article_saham.json\")\n",
    "table_article_saham = pd.concat([table_article_saham, table_article_saham_new]).reset_index(drop=True)\n",
    "\n",
    "article_related_to_saham = pd.concat([article_related_to_saham, article_related_to_saham_new]).reset_index(drop=True)\n",
    "\n",
    "article_norelated_to_saham = pd.read_json(\"data/article_norelated_to_saham.json\")\n",
    "article_norelated_to_saham = (pd.concat([article_norelated_to_saham, article_norelated_to_saham_new])\n",
    "                                  .reset_index(drop=True))\n",
    "\n",
    "# update data ke file, sebelum dilakukan update akan dilakukan\n",
    "# backup data terlebih dahulu ke folder backup\n",
    "# sehingga jika ada kesalahan datanya tidak akan hilang\n",
    "backup_data()\n",
    "table_article_saham.to_json('data/table_article_saham.json', orient=\"records\")\n",
    "table_article.to_json('data/table_articles.json', orient=\"records\")\n",
    "article_related_to_saham.to_json(\"data/article_related_to_saham.json\", orient=\"records\")\n",
    "article_norelated_to_saham.to_json(\"data/article_norelated_to_saham.json\", orient=\"records\")\n",
    "update_status_scraping_title(status_scraping_title)\n",
    "update_status_error(status_error)\n",
    "print(\"Data has been updated!\")\n",
    "update_data_saham()\n",
    "check_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_article = pd.read_json(\"data/table_articles.json\")\n",
    "tb_article = tb_article[tb_article.published_at.notnull()]\n",
    "tb_article.loc[:, 'date'] = tb_article.published_at.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "for date, df in tb_article.groupby(\"date\"):\n",
    "    df = df.drop(\"date\", axis=1)\n",
    "    df.to_json(f\"data/article/{date}.json\", orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
